{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a853ffc1",
   "metadata": {},
   "source": [
    "# Livrable 2 - Denoising d'images\n",
    "\n",
    "|Auteurs|\n",
    "|---|\n",
    "|Frédéric SPATARO|\n",
    "|Oscar PALISSOT|\n",
    "|Djayan DEMAISON|\n",
    "|Arnaud HITTINGER|\n",
    "|Nicolas PELLEGRINI|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8158e66b",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "## 1.1. Présentation du problème de débruitage d'image\n",
    "\n",
    "Le débruitage d'images, ou denoising, est une étape cruciale dans le prétraitement des données, en particulier dans le contexte des données numérisées. Les images numérisées, en particulier celles qui proviennent de documents papier, peuvent présenter divers degrés de bruit sous forme de points, de stries ou d'autres artefacts visuels qui peuvent potentiellement interférer avec les analyses ultérieures, telles que la classification ou la génération de légendes. Le débruitage est donc essentiel pour assurer la précision et la fiabilité des modèles de machine learning qui seront appliqués par la suite.\n",
    "\n",
    "## 1.2. Objectifs du Notebook\n",
    "\n",
    "L'objectif de ce notebook est de mettre en œuvre une solution de débruitage d'images en utilisant des auto-encodeurs convolutifs. Nous chercherons à :\n",
    "\n",
    "- Préparer les données d'images pour l'entraînement du modèle.\n",
    "- Construire et entraîner un auto-encodeur convolutif pour le débruitage d'images.\n",
    "- Évaluer les performances du modèle et visualiser les résultats.\n",
    "- Fournir une analyse critique des résultats et identifier les domaines potentiels d'amélioration.\n",
    "\n",
    "## 1.3. Briève introduction aux auto-encodeurs\n",
    "\n",
    "Les auto-encodeurs sont un type de réseau de neurones utilisé pour l'apprentissage non supervisé. Ils visent à apprendre une représentation (encodage) efficace des données, généralement dans le but de réduire la dimensionnalité. Un auto-encodeur est structuré en deux parties principales :\n",
    "\n",
    "- Encodeur: Il apprend à compresser l'entrée dans une forme latente, souvent de dimensionnalité réduite.\n",
    "- Décodeur: Il apprend à reconstruire l'entrée à partir de cette forme latente.\n",
    "\n",
    "Dans le contexte du débruitage d'images, les auto-encodeurs sont entraînés pour mapper des images bruitées à leurs versions propres, en apprenant à supprimer le bruit tout en conservant les caractéristiques importantes des données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87ff2d1",
   "metadata": {},
   "source": [
    "# 2. Importation et exploration des données\n",
    "\n",
    "## 2.1. Importation des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903bd23c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:15:49.460812600Z",
     "start_time": "2023-10-17T08:15:49.163465700Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#2\n",
    "from keras.preprocessing import image\n",
    "#3\n",
    "from sklearn.model_selection import train_test_split\n",
    "#4\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "#6\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "print(\"Import terminé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fd3f83",
   "metadata": {},
   "source": [
    "## 2.2. Chargement du dataset d'images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f685de98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:15:50.725501100Z",
     "start_time": "2023-10-17T08:15:49.172940300Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = 'DataL2'\n",
    "image_files = os.listdir(data_dir)\n",
    "\n",
    "# Paramètres des images\n",
    "img_height, img_width, rgb = 256, 256, 3\n",
    "images = []\n",
    "\n",
    "for img_file in image_files:\n",
    "    img_path = os.path.join(data_dir, img_file)\n",
    "    img = image.load_img(img_path, target_size=(256, 256))\n",
    "    img_array = image.img_to_array(img)\n",
    "    images.append(img_array)\n",
    "\n",
    "images = np.array(images)\n",
    "\n",
    "print(\"Chargement terminé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6d9280",
   "metadata": {},
   "source": [
    "## 2.3. Exploration des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca4b8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:15:50.794917700Z",
     "start_time": "2023-10-17T08:15:50.724696500Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Afficher quelques informations de base sur les données\n",
    "print(f\"Nombre total d'images : {images.shape[0]}\")\n",
    "print(f\"Dimensions des images : {images.shape[1:]}\")\n",
    "print(f\"Valeur min des pixels : {np.min(images)}\")\n",
    "print(f\"Valeur max des pixels : {np.max(images)}\")\n",
    "\n",
    "print(\"\\nExploration terminé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bae52b",
   "metadata": {},
   "source": [
    "## 2.4. Visualisation d'exemples d'images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8883c31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:15:51.346396100Z",
     "start_time": "2023-10-17T08:15:50.759001400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Afficher quelques images\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(images[i].astype('uint8'))\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Exemples d\\'Images du Dataset')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualisation terminé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e26926",
   "metadata": {},
   "source": [
    "## 2.5. Distribution des valeurs de pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168928e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:15:51.960261500Z",
     "start_time": "2023-10-17T08:15:51.345883600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculer la distribution des valeurs de pixel\n",
    "pixel_values = images.flatten()\n",
    "plt.hist(pixel_values, bins=50, color='blue', alpha=0.7, rwidth=0.8)\n",
    "plt.title('Distribution des valeurs de pixel dans le dataset d\\'images')\n",
    "plt.xlabel('Valeur de Pixel')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()\n",
    "\n",
    "# Calculer et afficher la moyenne et l'écart-type des valeurs de pixel\n",
    "mean_pixel_value = np.mean(pixel_values)\n",
    "std_pixel_value = np.std(pixel_values)\n",
    "print(f\"La valeur moyenne des pixels est: {mean_pixel_value:.2f}\")\n",
    "print(f\"L'écart-type des valeurs de pixel est: {std_pixel_value:.2f}\")\n",
    "\n",
    "print(\"Visualisation terminée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7067d4e",
   "metadata": {},
   "source": [
    "* Moyenne des pixels :\n",
    "\n",
    "La valeur moyenne des pixels de 117.62 suggère que, sur une échelle de 0 (noir) à 255 (blanc), nos images tendent à être d'une luminosité modérée à élevée. Cela pourrait signifier que nos images sont relativement claires, mais avec une présence notable de zones plus sombres puisque la moyenne n'est pas extrêmement élevée.\n",
    "\n",
    "* Écart-Type des Pixels : \n",
    "\n",
    "Un écart-type de 71.57 indique une assez grande variabilité dans les valeurs des pixels à travers l'ensemble des images. Cela signifie que bien que la moyenne des valeurs de pixel soit relativement élevée, il y a une quantité significative de pixels qui sont soit beaucoup plus clairs, soit beaucoup plus sombres que la moyenne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5bdb17",
   "metadata": {},
   "source": [
    "# 3. Préparation des données\n",
    "\n",
    "## 3.1. Normalisation des images\n",
    "\n",
    "On divise toutes les valeurs de pixels par 255, ce qui transforme les valeurs de pixel originales de la plage [0, 255] à [0, 1]. Cette étape permet de modérer les poids et gradients lors de l'entraînement du réseau de neurones, facilitant ainsi la convergence du modèle et améliorant la stabilité du processus d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa11c476",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:15:51.979353200Z",
     "start_time": "2023-10-17T08:15:51.959669400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normaliser les images pour que les valeurs de pixel soient dans [0, 1]\n",
    "normalized_images = images / 255.0\n",
    "\n",
    "print(\"Normalisation terminé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b414c",
   "metadata": {},
   "source": [
    "## 3.2. Bruitage artificiel des images\n",
    "\n",
    "On introduit un bruit artificiel dans les données d'entraînement pour forcer le modèle à apprendre à reconstruire l'image originale à partir de sa version bruitée. \n",
    "\n",
    "On a choisi une `noise_factor` de `0.15` après une série d'expérimentations et d'observations. Ce niveau de bruit est suffisant pour introduire un défi pour l'auto-encodeur tout en préservant les caractéristiques clés des images pour que le modèle puisse apprendre efficacement à les reconstruire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cd7399",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:15:53.135193100Z",
     "start_time": "2023-10-17T08:15:51.983354300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Paramètres pour l'ajout de bruit\n",
    "noise_factor = 0.15\n",
    "\n",
    "# Ajout de bruit gaussien aux images\n",
    "noisy_images = normalized_images + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=normalized_images.shape)\n",
    "noisy_images = np.clip(noisy_images, 0., 1.)\n",
    "\n",
    "print(\"Bruitage terminé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb55c64",
   "metadata": {},
   "source": [
    "## 3.3. Division en série d'entraînement et de test\n",
    "\n",
    "On a opté pour une répartition de 80% des données pour l'entraînement et 20% pour le test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97527d08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:15:53.228661Z",
     "start_time": "2023-10-17T08:15:53.134650900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    noisy_images,  # les images bruitées -> entrées\n",
    "    normalized_images,  # les images originales -> cibles\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Division en test/train terminé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed15050",
   "metadata": {},
   "source": [
    "## 3.4. Visualisation des images bruitées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed13820a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:15:54.014961200Z",
     "start_time": "2023-10-17T08:15:53.225133800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Afficher quelques images bruitées\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(noisy_images[i].astype('float32'))\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Exemples d\\'images Bruitées')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualisation terminé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa5f539",
   "metadata": {},
   "source": [
    "# 4. Construction de l'auto-encodeur convolutif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f26105",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:15:54.014961200Z",
     "start_time": "2023-10-17T08:15:54.013961500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Définition des images\n",
    "input_img = Input(shape=(img_height, img_width, rgb))\n",
    "\n",
    "print(\"Paramètres appliqués.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fc3333",
   "metadata": {},
   "source": [
    "## 4.1. Architecture de l'encodeur\n",
    "\n",
    "On utilise ici une série de couches convolutives et de pooling pour progressivement réduire la dimensionnalité de l'entrée. Les couches convolutives extraient les caractéristiques spatiales des images, tandis que les couches de pooling réduisent les dimensions spatiales (hauteur et largeur), compactant ainsi l'information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9785171d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:15:54.153692800Z",
     "start_time": "2023-10-17T08:15:54.014961200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Paramètres de l'encodeur\n",
    "encoder_params = [\n",
    "    {\"filters\": 64, \"kernel_size\": (3, 3), \"activation\": 'relu', \"padding\": 'same'},\n",
    "    {\"pool_size\": (2, 2), \"padding\": 'same'},\n",
    "    {\"filters\": 128, \"kernel_size\": (3, 3), \"activation\": 'relu', \"padding\": 'same'},\n",
    "    {\"pool_size\": (2, 2), \"padding\": 'same'}\n",
    "]\n",
    "\n",
    "# Encodeur\n",
    "x = input_img\n",
    "for i in range(0, len(encoder_params), 2):\n",
    "    x = Conv2D(**encoder_params[i])(x)\n",
    "    x = MaxPooling2D(**encoder_params[i+1])(x)\n",
    "encoded = x\n",
    "\n",
    "print(\"Encodeur paramétré.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757f4eae",
   "metadata": {},
   "source": [
    "## 4.2. Architecture du décodeur\n",
    "\n",
    "On cherche à reconstruire l'image d'origine via un série de couches convolutives et d'opérations d'upsampling. Les couches convolutives dans le décodeur servent à générer de nouvelles caractéristiques à partir de l'espace latent, tandis que les couches d'upsampling augmentent progressivement la hauteur et la largeur des volumes de sortie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedd00f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:15:54.174307400Z",
     "start_time": "2023-10-17T08:15:54.078745500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Paramètres du décodeur\n",
    "decoder_params = [\n",
    "    {\"filters\": 128, \"kernel_size\": (3, 3), \"activation\": 'relu', \"padding\": 'same'},\n",
    "    {\"size\": (2, 2)},\n",
    "    {\"filters\": 64, \"kernel_size\": (3, 3), \"activation\": 'relu', \"padding\": 'same'},\n",
    "    {\"size\": (2, 2)},\n",
    "    {\"filters\": rgb, \"kernel_size\": (3, 3), \"activation\": 'sigmoid', \"padding\": 'same'}\n",
    "]\n",
    "\n",
    "# Décodeur\n",
    "for i in range(0, len(decoder_params)-1, 2):\n",
    "    x = Conv2D(**decoder_params[i])(x)\n",
    "    x = UpSampling2D(**decoder_params[i+1])(x)\n",
    "x = Conv2D(**decoder_params[-1])(x)\n",
    "decoded = x\n",
    "\n",
    "print(\"Décodeur paramétré.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dd9d0b",
   "metadata": {},
   "source": [
    "## 4.3. Modèle d'auto-encodeur complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f96919",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:15:54.260995400Z",
     "start_time": "2023-10-17T08:15:54.175869800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Construction du modèle d'auto-encodeur complet\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.summary()\n",
    "\n",
    "# Augmentation des données\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fae9433",
   "metadata": {},
   "source": [
    "# 5. Entraînement du modèle\n",
    "\n",
    "## 5.1. Définition des hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf58920",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:15:54.269998100Z",
     "start_time": "2023-10-17T08:15:54.239996700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Paramètres globaux\n",
    "epochs = 150\n",
    "batch_size = 4\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    restore_best_weights=True)\n",
    "\n",
    "print(\"Hyperparamètres définis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e709c4",
   "metadata": {},
   "source": [
    "## 5.2. Entraînement de l'auto-encodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60af8689",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:19:53.461778700Z",
     "start_time": "2023-10-17T08:15:54.254999800Z"
    }
   },
   "outputs": [],
   "source": [
    "history = autoencoder.fit(\n",
    "    X_train,  # images bruitées\n",
    "    y_train,  # images originales\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "print(\"Auto-encodeur entraîné.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da14c276",
   "metadata": {},
   "source": [
    "## 5.3. Visualisation de la loss durant l'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21908215",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:19:53.643729200Z",
     "start_time": "2023-10-17T08:19:53.476724100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tracer l'historique de la perte durant l'entraînement\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Test Loss')\n",
    "plt.title('Évolution de la Perte durant l\\'Entraînement')\n",
    "plt.xlabel('Époque')\n",
    "plt.ylabel('Perte')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Loss visualisé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed399e79",
   "metadata": {},
   "source": [
    "# 6. Évaluation du modèle\n",
    "\n",
    "## 6.1. Évaluation quantitative du modèle sur l'ensemble de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75865451",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:19:53.886385700Z",
     "start_time": "2023-10-17T08:19:53.646313400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prédiction des images débruitées à partir des images bruitées de test\n",
    "decoded_imgs = autoencoder.predict(X_test)\n",
    "\n",
    "# Initialisation des métriques\n",
    "psnr_vals = []\n",
    "\n",
    "# Boucle sur les images pour calculer PSNR\n",
    "for orig, pred in zip(y_test, decoded_imgs):\n",
    "    orig = orig.astype(\"float32\")\n",
    "    pred = pred.astype(\"float32\")\n",
    "    \n",
    "    # Calcul et stockage du PSNR pour chaque paire d'images\n",
    "    psnr_vals.append(psnr(orig, pred, data_range=orig.max() - orig.min()))\n",
    "\n",
    "# Calcul de la moyenne du PSNR\n",
    "mean_psnr = np.mean(psnr_vals)\n",
    "\n",
    "# Affichage du PSNR moyen\n",
    "print(f\"Mean PSNR: {mean_psnr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5ab077",
   "metadata": {},
   "source": [
    "## 6.2. Visualisation des images débruitées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf4da49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:20:15.632731200Z",
     "start_time": "2023-10-17T08:19:53.885861700Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prédiction des images débruitées à partir des images bruitées\n",
    "decoded_imgs = autoencoder.predict(X_test)\n",
    "\n",
    "# Affichage des images originales, bruitées et débruitées\n",
    "plt.figure(figsize=(300, 140))\n",
    "num_images_to_show = 6\n",
    "\n",
    "for i in range(num_images_to_show):\n",
    "    # Images originales\n",
    "    ax = plt.subplot(3, num_images_to_show, i + 1)\n",
    "    plt.imshow(y_test[i].reshape(img_height, img_width, rgb))\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # Images bruitées\n",
    "    ax = plt.subplot(3, num_images_to_show, i + 1 + num_images_to_show)\n",
    "    plt.imshow(X_test[i].reshape(img_height, img_width, rgb))\n",
    "    plt.title(\"Noisy\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # Images débruitées générées par l'auto-encodeur\n",
    "    ax = plt.subplot(3, num_images_to_show, i + 1 + 2*num_images_to_show)\n",
    "    plt.imshow(decoded_imgs[i].reshape(img_height, img_width, rgb))\n",
    "    plt.title(\"Denoised\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Image Reconstruction by Autoencoder\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
