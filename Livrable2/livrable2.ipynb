{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a853ffc1",
   "metadata": {},
   "source": [
    "# Livrable 2 - Denoising d'images\n",
    "\n",
    "|Auteurs|\n",
    "|---|\n",
    "|Frédéric SPATARO|\n",
    "|Oscar PALISSOT|\n",
    "|Djayan DEMAISON|\n",
    "|Arnaud HITTINGER|\n",
    "|Nicolas PELLEGRINI|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8158e66b",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "## 1.1. Présentation du problème de débruitage d'image\n",
    "\n",
    "Le débruitage d'images, ou denoising, est une étape cruciale dans le prétraitement des données, en particulier dans le contexte des données numérisées. Les images numérisées, en particulier celles qui proviennent de documents papier, peuvent présenter divers degrés de bruit sous forme de points, de stries ou d'autres artefacts visuels qui peuvent potentiellement interférer avec les analyses ultérieures, telles que la classification ou la génération de légendes. Le débruitage est donc essentiel pour assurer la précision et la fiabilité des modèles de machine learning qui seront appliqués par la suite.\n",
    "\n",
    "## 1.2. Objectifs du Notebook\n",
    "\n",
    "L'objectif de ce notebook est de mettre en œuvre une solution de débruitage d'images en utilisant des auto-encodeurs convolutifs. Nous chercherons à :\n",
    "\n",
    "- Préparer les données d'images pour l'entraînement du modèle.\n",
    "- Construire et entraîner un auto-encodeur convolutif pour le débruitage d'images.\n",
    "- Évaluer les performances du modèle et visualiser les résultats.\n",
    "- Fournir une analyse critique des résultats et identifier les domaines potentiels d'amélioration.\n",
    "\n",
    "## 1.3. Briève introduction aux auto-encodeurs\n",
    "\n",
    "Les auto-encodeurs sont un type de réseau de neurones utilisé pour l'apprentissage non supervisé. Ils visent à apprendre une représentation (encodage) efficace des données, généralement dans le but de réduire la dimensionnalité. Un auto-encodeur est structuré en deux parties principales :\n",
    "\n",
    "- Encodeur: Il apprend à compresser l'entrée dans une forme latente, souvent de dimensionnalité réduite.\n",
    "- Décodeur: Il apprend à reconstruire l'entrée à partir de cette forme latente.\n",
    "\n",
    "Dans le contexte du débruitage d'images, les auto-encodeurs sont entraînés pour mapper des images bruitées à leurs versions propres, en apprenant à supprimer le bruit tout en conservant les caractéristiques importantes des données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87ff2d1",
   "metadata": {},
   "source": [
    "# 2. Importation et exploration des données\n",
    "\n",
    "## 2.1. Importation des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903bd23c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T09:17:08.483286600Z",
     "start_time": "2023-10-17T09:17:08.318290900Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#2\n",
    "from keras.preprocessing import image\n",
    "#3\n",
    "from sklearn.model_selection import train_test_split\n",
    "#4\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "#6\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "print(\"Import terminé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fd3f83",
   "metadata": {},
   "source": [
    "## 2.2. Chargement du dataset d'images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f685de98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T09:17:08.869291500Z",
     "start_time": "2023-10-17T09:17:08.327326Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = 'DataL2'\n",
    "image_files = os.listdir(data_dir)\n",
    "\n",
    "# Paramètres des images\n",
    "img_height, img_width, rgb = 256, 256, 3\n",
    "images = []\n",
    "\n",
    "for img_file in image_files:\n",
    "    img_path = os.path.join(data_dir, img_file)\n",
    "    img = image.load_img(img_path, target_size=(256, 256))\n",
    "    img_array = image.img_to_array(img)\n",
    "    images.append(img_array)\n",
    "\n",
    "images = np.array(images)\n",
    "\n",
    "print(\"Chargement terminé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6d9280",
   "metadata": {},
   "source": [
    "## 2.3. Exploration des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca4b8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T09:17:08.975530600Z",
     "start_time": "2023-10-17T09:17:08.870291500Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Afficher quelques informations de base sur les données\n",
    "print(f\"Nombre total d'images : {images.shape[0]}\")\n",
    "print(f\"Dimensions des images : {images.shape[1:]}\")\n",
    "print(f\"Valeur min des pixels : {np.min(images)}\")\n",
    "print(f\"Valeur max des pixels : {np.max(images)}\")\n",
    "\n",
    "print(\"\\nExploration terminé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bae52b",
   "metadata": {},
   "source": [
    "## 2.4. Visualisation d'exemples d'images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8883c31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T09:17:09.654926Z",
     "start_time": "2023-10-17T09:17:08.902933300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Afficher quelques images\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(images[i].astype('uint8'))\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Exemples d\\'Images du Dataset')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualisation terminé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e26926",
   "metadata": {},
   "source": [
    "## 2.5. Distribution des valeurs de pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168928e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T09:17:10.396429600Z",
     "start_time": "2023-10-17T09:17:09.642235700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculer la distribution des valeurs de pixel\n",
    "pixel_values = images.flatten()\n",
    "plt.hist(pixel_values, bins=50, color='blue', alpha=0.7, rwidth=0.8)\n",
    "plt.title('Distribution des valeurs de pixel dans le dataset d\\'images')\n",
    "plt.xlabel('Valeur de Pixel')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()\n",
    "\n",
    "# Calculer et afficher la moyenne et l'écart-type des valeurs de pixel\n",
    "mean_pixel_value = np.mean(pixel_values)\n",
    "std_pixel_value = np.std(pixel_values)\n",
    "print(f\"La valeur moyenne des pixels est: {mean_pixel_value:.2f}\")\n",
    "print(f\"L'écart-type des valeurs de pixel est: {std_pixel_value:.2f}\")\n",
    "\n",
    "print(\"Visualisation terminée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7067d4e",
   "metadata": {},
   "source": [
    "* Moyenne des pixels :\n",
    "\n",
    "La valeur moyenne des pixels de 117.62 suggère que, sur une échelle de 0 (noir) à 255 (blanc), nos images tendent à être d'une luminosité modérée à élevée. Cela pourrait signifier que nos images sont relativement claires, mais avec une présence notable de zones plus sombres puisque la moyenne n'est pas extrêmement élevée.\n",
    "\n",
    "* Écart-Type des Pixels : \n",
    "\n",
    "Un écart-type de 71.57 indique une assez grande variabilité dans les valeurs des pixels à travers l'ensemble des images. Cela signifie que bien que la moyenne des valeurs de pixel soit relativement élevée, il y a une quantité significative de pixels qui sont soit beaucoup plus clairs, soit beaucoup plus sombres que la moyenne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5bdb17",
   "metadata": {},
   "source": [
    "# 3. Préparation des données\n",
    "\n",
    "## 3.1. Normalisation des images\n",
    "\n",
    "On divise toutes les valeurs de pixels par 255, ce qui transforme les valeurs de pixel originales de la plage [0, 255] à [0, 1]. Cette étape permet de modérer les poids et gradients lors de l'entraînement du réseau de neurones, facilitant ainsi la convergence du modèle et améliorant la stabilité du processus d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa11c476",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T09:17:10.396429600Z",
     "start_time": "2023-10-17T09:17:10.363406200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normaliser les images pour que les valeurs de pixel soient dans [0, 1]\n",
    "normalized_images = images / 255.0\n",
    "\n",
    "print(\"Normalisation terminé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b414c",
   "metadata": {},
   "source": [
    "## 3.2. Bruitage artificiel des images\n",
    "\n",
    "On introduit un bruit artificiel dans les données d'entraînement pour forcer le modèle à apprendre à reconstruire l'image originale à partir de sa version bruitée. \n",
    "\n",
    "On a choisi une `noise_factor` de `0.15` après une série d'expérimentations et d'observations. Ce niveau de bruit est suffisant pour introduire un défi pour l'auto-encodeur tout en préservant les caractéristiques clés des images pour que le modèle puisse apprendre efficacement à les reconstruire. \n",
    "\n",
    "On va également tester un `noise_factor` de `0.40` afin de vérifier la puissance du modèle en utilisant un bruit nettement plus fort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cd7399",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T09:17:11.247375100Z",
     "start_time": "2023-10-17T09:17:10.381433100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Paramètres pour l'ajout de bruit\n",
    "noise_factor_015 = 0.15\n",
    "noise_factor_040 = 0.40\n",
    "\n",
    "# Ajout d'un bruit gaussien léger aux images\n",
    "noisy_images_015 = normalized_images + noise_factor_015 * np.random.normal(loc=0.0, scale=1.0, size=normalized_images.shape)\n",
    "noisy_images_015 = np.clip(noisy_images_015, 0., 1.)\n",
    "\n",
    "# Ajout d'un bruit gaussien fort aux images\n",
    "noisy_images_040 = normalized_images + noise_factor_040 * np.random.normal(loc=0.0, scale=1.0, size=normalized_images.shape)\n",
    "noisy_images_040 = np.clip(noisy_images_040, 0., 1.)\n",
    "\n",
    "print(\"Bruitage terminé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb55c64",
   "metadata": {},
   "source": [
    "## 3.3. Division en série d'entraînement et de test\n",
    "\n",
    "On a opté pour une répartition de 80% des données pour l'entraînement et 20% pour le test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97527d08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T09:17:11.302960900Z",
     "start_time": "2023-10-17T09:17:11.243376700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train_015, X_test_015, y_train_015, y_test_015 = train_test_split(\n",
    "    noisy_images_015,   # Entrée (images bruitées)\n",
    "    normalized_images,  # Cibles (images originales)\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Jeux de données constitués\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ecaa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train_040, X_test_040, y_train_040, y_test_040 = train_test_split(\n",
    "    noisy_images_040,   # Entrée (images bruitées)\n",
    "    normalized_images,  # Cibles (images originales)\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Jeux de données constitués\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed15050",
   "metadata": {},
   "source": [
    "## 3.4. Visualisation des images bruitées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed13820a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T09:17:11.894105200Z",
     "start_time": "2023-10-17T09:17:11.298180900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Afficher quelques images légèrement bruitées\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(noisy_images_015[i].astype('float32'))\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Exemples d\\'images légèrement bruitées')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7887679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher quelques images fortement bruitées\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(noisy_images_040[i].astype('float32'))\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Exemples d\\'images fortement bruitées')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa5f539",
   "metadata": {},
   "source": [
    "# 4. Construction de l'auto-encodeur convolutif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f26105",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T09:17:11.894105200Z",
     "start_time": "2023-10-17T09:17:11.884724100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Définition des images\n",
    "input_img = Input(shape=(img_height, img_width, rgb))\n",
    "\n",
    "print(\"Paramètres appliqués.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fc3333",
   "metadata": {},
   "source": [
    "## 4.1. Architecture de l'encodeur\n",
    "\n",
    "On utilise ici une série de couches convolutives et de pooling pour progressivement réduire la dimensionnalité de l'entrée. Les couches convolutives extraient les caractéristiques spatiales des images, tandis que les couches de pooling réduisent les dimensions spatiales (hauteur et largeur), compactant ainsi l'information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9785171d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T09:17:11.894105200Z",
     "start_time": "2023-10-17T09:17:11.884724100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Paramètres de l'encodeur\n",
    "encoder_params = [\n",
    "    {\"filters\": 64, \"kernel_size\": (3, 3), \"activation\": 'relu', \"padding\": 'same'},\n",
    "    {\"pool_size\": (2, 2), \"padding\": 'same'},\n",
    "    {\"filters\": 128, \"kernel_size\": (3, 3), \"activation\": 'relu', \"padding\": 'same'},\n",
    "    {\"pool_size\": (2, 2), \"padding\": 'same'}\n",
    "]\n",
    "\n",
    "# Encodeur\n",
    "x = input_img\n",
    "for i in range(0, len(encoder_params), 2):\n",
    "    x = Conv2D(**encoder_params[i])(x)\n",
    "    x = MaxPooling2D(**encoder_params[i+1])(x)\n",
    "encoded = x\n",
    "\n",
    "print(\"Encodeur paramétré.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757f4eae",
   "metadata": {},
   "source": [
    "## 4.2. Architecture du décodeur\n",
    "\n",
    "On cherche à reconstruire l'image d'origine via un série de couches convolutives et d'opérations d'upsampling. Les couches convolutives dans le décodeur servent à générer de nouvelles caractéristiques à partir de l'espace latent, tandis que les couches d'upsampling augmentent progressivement la hauteur et la largeur des volumes de sortie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedd00f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T09:17:11.923124300Z",
     "start_time": "2023-10-17T09:17:11.884724100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Paramètres du décodeur\n",
    "decoder_params = [\n",
    "    {\"filters\": 128, \"kernel_size\": (3, 3), \"activation\": 'relu', \"padding\": 'same'},\n",
    "    {\"size\": (2, 2)},\n",
    "    {\"filters\": 64, \"kernel_size\": (3, 3), \"activation\": 'relu', \"padding\": 'same'},\n",
    "    {\"size\": (2, 2)},\n",
    "    {\"filters\": rgb, \"kernel_size\": (3, 3), \"activation\": 'sigmoid', \"padding\": 'same'}\n",
    "]\n",
    "\n",
    "# Décodeur\n",
    "for i in range(0, len(decoder_params)-1, 2):\n",
    "    x = Conv2D(**decoder_params[i])(x)\n",
    "    x = UpSampling2D(**decoder_params[i+1])(x)\n",
    "x = Conv2D(**decoder_params[-1])(x)\n",
    "decoded = x\n",
    "\n",
    "print(\"Décodeur paramétré.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dd9d0b",
   "metadata": {},
   "source": [
    "## 4.3. Modèle d'auto-encodeur complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f96919",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T09:17:12.093633500Z",
     "start_time": "2023-10-17T09:17:11.936238600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Construction du modèle d'auto-encodeur complet\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.summary()\n",
    "\n",
    "# Augmentation des données\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fae9433",
   "metadata": {},
   "source": [
    "# 5. Entraînement du modèle\n",
    "\n",
    "## 5.1. Définition des hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf58920",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T09:17:12.094634900Z",
     "start_time": "2023-10-17T09:17:11.971183500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Paramètres globaux\n",
    "epochs = 150\n",
    "batch_size = 4\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    restore_best_weights=True)\n",
    "\n",
    "print(\"Hyperparamètres définis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e709c4",
   "metadata": {},
   "source": [
    "## 5.2. Entraînement de l'auto-encodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60af8689",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T09:18:23.037413300Z",
     "start_time": "2023-10-17T09:17:12.075493300Z"
    }
   },
   "outputs": [],
   "source": [
    "history_015 = autoencoder.fit(\n",
    "    X_train_015,  # images bruitées\n",
    "    y_train_015,  # images originales\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test_015, y_test_015),\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "print(\"Auto-encodeur 1 entraîné.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04626de",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_040 = autoencoder.fit(\n",
    "    X_train_040,  # images bruitées\n",
    "    y_train_040,  # images originales\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test_040, y_test_040),\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "print(\"Auto-encodeur 2 entraîné.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da14c276",
   "metadata": {},
   "source": [
    "## 5.3. Visualisation de la loss durant l'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21908215",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T09:18:23.197551300Z",
     "start_time": "2023-10-17T09:18:23.064806400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tracer l'historique de la perte durant l'entraînement\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history_015.history['loss'], label='Train loss')\n",
    "plt.plot(history_015.history['val_loss'], label='Test loss')\n",
    "plt.title('Évolution de la perte durant l\\'entraînement')\n",
    "plt.xlabel('Époque')\n",
    "plt.ylabel('Perte')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Loss visualisé.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75696caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracer l'historique de la perte durant l'entraînement\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history_040.history['loss'], label='Train loss')\n",
    "plt.plot(history_040.history['val_loss'], label='Test loss')\n",
    "plt.title('Évolution de la perte durant l\\'entraînement')\n",
    "plt.xlabel('Époque')\n",
    "plt.ylabel('Perte')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Loss visualisé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed399e79",
   "metadata": {},
   "source": [
    "# 6. Évaluation du modèle\n",
    "\n",
    "## 6.1. Évaluation quantitative du modèle sur l'ensemble de test via PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75865451",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T09:18:23.420482100Z",
     "start_time": "2023-10-17T09:18:23.200366400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prédiction des images débruitées à partir des images bruitées de test\n",
    "decoded_imgs_015 = autoencoder.predict(X_test_015)\n",
    "decoded_imgs_040 = autoencoder.predict(X_test_040)\n",
    "\n",
    "# Initialisation des métriques\n",
    "psnr_vals = []\n",
    "\n",
    "# Calcul du PNSR pour les images ayant été légèrement bruitées\n",
    "for orig, pred in zip(y_test_015, decoded_imgs_015):\n",
    "    orig = orig.astype(\"float32\")\n",
    "    pred = pred.astype(\"float32\")\n",
    "    \n",
    "    # Calcul et stockage du PSNR pour chaque paire d'images\n",
    "    psnr_vals.append(psnr(orig, pred, data_range=orig.max() - orig.min()))\n",
    "\n",
    "# Calcul de la moyenne du PSNR\n",
    "mean_psnr = np.mean(psnr_vals)\n",
    "print(f\"Mean PSNR (0.15): {mean_psnr:.2f}\")\n",
    "\n",
    "# Réinitialisation des métriques\n",
    "psnr_vals = []\n",
    "\n",
    "# Calcul du PNSR pour les images ayant été fortement bruitées\n",
    "for orig, pred in zip(y_test_040, decoded_imgs_040):\n",
    "    orig = orig.astype(\"float32\")\n",
    "    pred = pred.astype(\"float32\")\n",
    "    \n",
    "    # Calcul et stockage du PSNR pour chaque paire d'images\n",
    "    psnr_vals.append(psnr(orig, pred, data_range=orig.max() - orig.min()))\n",
    "\n",
    "# Calcul de la moyenne du PSNR\n",
    "mean_psnr = np.mean(psnr_vals)\n",
    "print(f\"Mean PSNR (0.40): {mean_psnr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a610f",
   "metadata": {},
   "source": [
    "## 6.2. Évaluation quantitative du modèle sur l'ensemble de test via MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca794e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5ab077",
   "metadata": {},
   "source": [
    "## 6.3. Visualisation des images débruitées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf4da49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T09:18:45.305903400Z",
     "start_time": "2023-10-17T09:18:23.421530500Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Affichage des images originales, bruitées et débruitées\n",
    "plt.figure(figsize=(300, 140))\n",
    "num_images_to_show = 6\n",
    "\n",
    "for i in range(num_images_to_show):\n",
    "    # Images originales\n",
    "    ax = plt.subplot(3, num_images_to_show, i + 1)\n",
    "    plt.imshow(y_test_015[i].reshape(img_height, img_width, rgb))\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # Images légèrement bruitées\n",
    "    ax = plt.subplot(3, num_images_to_show, i + 1 + num_images_to_show)\n",
    "    plt.imshow(X_test_015[i].reshape(img_height, img_width, rgb))\n",
    "    plt.title(\"Noisy (0.15)\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Images fortement bruitées\n",
    "    ax = plt.subplot(3, num_images_to_show, i + 1 + num_images_to_show)\n",
    "    plt.imshow(X_test_040[i].reshape(img_height, img_width, rgb))\n",
    "    plt.title(\"Noisy (0.40)\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # Images légèrement bruitées débruitées générées par l'auto-encodeur\n",
    "    ax = plt.subplot(3, num_images_to_show, i + 1 + 2*num_images_to_show)\n",
    "    plt.imshow(decoded_imgs_015[i].reshape(img_height, img_width, rgb))\n",
    "    plt.title(\"Denoised (0.15)\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Images fortement bruitées débruitées générées par l'auto-encodeur\n",
    "    ax = plt.subplot(3, num_images_to_show, i + 1 + 2*num_images_to_show)\n",
    "    plt.imshow(decoded_imgs_040[i].reshape(img_height, img_width, rgb))\n",
    "    plt.title(\"Denoised (0.40)\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Image reconstruction by autoencoder\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
