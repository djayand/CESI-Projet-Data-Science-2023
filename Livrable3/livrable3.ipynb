{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d97ab6a6",
   "metadata": {},
   "source": [
    "# Livrable 3 - Captionning d'images\n",
    "\n",
    "|Auteurs|\n",
    "|---|\n",
    "|Frédéric SPATARO|\n",
    "|Oscar PALISSOT|\n",
    "|Djayan DEMAISON|\n",
    "|Arnaud HITTINGER|\n",
    "|Nicolas PELLEGRINI|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e896510b",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "## 1.1. Besoins du projet\n",
    "\n",
    "L'objectif principal de ce projet est de développer un système automatique de génération de légendes pour des images. Cette tâche est cruciale pour l'entreprise TouNum, qui vise à étendre ses services de numérisation avec des fonctionnalités d'intelligence artificielle. Le \"captioning\" d'images, ou la génération automatique de légendes, permettra d'ajouter une valeur significative aux services existants, en facilitant la catégorisation et la recherche d'images dans les bases de données numérisées.\n",
    "\n",
    "## 1.2. Dataset \"MS COCO\"\n",
    "\n",
    "Pour atteindre cet objectif, nous utiliserons le dataset MS COCO (Microsoft Common Objects in Context), qui est largement reconnu dans la communauté de l'apprentissage automatique pour l'entraînement et l'évaluation des modèles de captioning d'images. Ce dataset contient plus de 200,000 images annotées avec environ 1.2 million de légendes, fournissant ainsi une ressource riche pour l'entraînement de notre modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8a8b6c",
   "metadata": {},
   "source": [
    "# 2. Installation et importation \n",
    "\n",
    "## 2.1. Installation des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab628dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install -U scikit-learn scipy matplotlib\n",
    "!pip install numpy\n",
    "!pip install Pillow\n",
    "!pip install tqdm\n",
    "\n",
    "print(\"Installation terminée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10586ddc4e0417b6",
   "metadata": {},
   "source": [
    "## 2.2. Importation des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fca277094268aa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T06:53:54.465275400Z",
     "start_time": "2023-10-26T06:53:43.980862900Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import collections\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import zipfile\n",
    "\n",
    "print(\"Importation terminée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8f41b0cfff98d4",
   "metadata": {},
   "source": [
    "# 3. Chargement des données\n",
    "\n",
    "## 3.1. Dézipage des données\n",
    "\n",
    "On va décompresser les données stockées au format ZIP du dataset MS COCO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6741f00eb709c661",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T06:55:58.053792800Z",
     "start_time": "2023-10-26T06:53:54.469626600Z"
    }
   },
   "outputs": [],
   "source": [
    "repertoire = './Train2014Zip'\n",
    "dossier_dataset = './train2014'\n",
    "if not os.path.exists(dossier_dataset):\n",
    "    os.makedirs(dossier_dataset)\n",
    "\n",
    "for fichier in os.listdir(repertoire):\n",
    "    chembin_fichier = os.path.join(repertoire,fichier)\n",
    "    if zipfile.is_zipfile(chemin_fichier):\n",
    "        with zipfile.ZipFile(chemin_fichier, 'r') as zip_ref:\n",
    "            zip_ref.extractall(dossier_dataset)\n",
    "            print(f'Fichier {fichier} décompressé dans le dossier Dataset.')\n",
    "\n",
    "print(\"Tous les fichiers ZIP ont été décompressés.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97ed3af",
   "metadata": {},
   "source": [
    "## 3.2. Définition des chemins d'accès aux données\n",
    "\n",
    "On définit les chemins d'accès aux :\n",
    "\n",
    "* Fichier d'annotations\n",
    "* Dossier des images du dataset à annoter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8771aced2726b0d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T06:55:58.060116900Z",
     "start_time": "2023-10-26T06:55:58.055752600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Chemin du fichier d'annotations\n",
    "annotation_folder = os.path.abspath('.')+\"/annotations/\"\n",
    "annotation_file = annotation_folder+\"captions_train2014.json\"\n",
    "\n",
    "# Chemin du dossier contenant les images à annoter\n",
    "image_folder = '/train2014/train2014/'\n",
    "PATH = os.path.abspath('.') + image_folder\n",
    "\n",
    "print(\"Fichier d'annotation : \", annotation_file)\n",
    "print(\"Le chemin PATH : \", PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e99a4d",
   "metadata": {},
   "source": [
    "## 3.3. Chargement des annotations et des instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee0dd43940945d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T06:56:01.618070400Z",
     "start_time": "2023-10-26T06:55:58.061411600Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(annotation_folder+\"/captions_val2014.json\", 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "with open(annotation_folder+\"instances_val2014.json\", 'r') as f:\n",
    "    instances = json.load(f)\n",
    "    \n",
    "print(\"Annotations et instances chargées.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab624cd0",
   "metadata": {},
   "source": [
    "## 3.4. Organisation et préparation des annotations\n",
    "\n",
    "On s'occupe de la préparation et de l'organisation des annotations textuelles associées à chaque image. Le but est d'avoir une structure organisée où chaque chemin d'accès à une image est associé à une liste de ses annotations. Cela permettra une utilisation efficace des données pendant le processus d'entraînement du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bc87c73615b55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T06:56:02.685427Z",
     "start_time": "2023-10-26T06:56:01.620860500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lecture du fichier d'annotation\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Grouper toutes les annotations ayant le meme identifiant.\n",
    "image_path_to_caption = collections.defaultdict(list)\n",
    "for val in annotations['annotations']:\n",
    "    # marquer le debut et la fin de chaque annotation\n",
    "    caption = f\"<start> {val['caption']} <end>\"\n",
    "    # L'identifiant d'une image fait partie de son chemin d'accès\n",
    "    image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])\n",
    "    # Rajout du caption associé à image_path\n",
    "    image_path_to_caption[image_path].append(caption)\n",
    "    \n",
    "print(\"Préparation terminée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2969d4f2",
   "metadata": {},
   "source": [
    "## 3.5. Sélection d'un sous-ensemble pour l'entraînement\n",
    "\n",
    "Les chemins d'accès à toutes les images sont mélangés aléatoirement. Cela garantit que la sélection d'images pour l'entraînement est variée et non biaisée. Seuls les chemins des 10 000 premières images sont sélectionnés pour l'entraînement après le mélange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e2d5050af6a69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T06:56:02.731987600Z",
     "start_time": "2023-10-26T06:56:02.697400100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prendre les premières images seulement\n",
    "image_paths = list(image_path_to_caption.keys())\n",
    "random.shuffle(image_paths)\n",
    "train_image_paths = image_paths[:10000]\n",
    "\n",
    "print(\"Nombres d'images utilisées :\", len(train_image_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e5aa86d6a05cfc",
   "metadata": {},
   "source": [
    "## 3.6. Préparation des listes pour l'entraînement\n",
    "\n",
    "On organise et on prépare les données nécessaires à l'entraînement du modèle de légendage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca877999a1d3177b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T06:56:02.748418100Z",
     "start_time": "2023-10-26T06:56:02.737062800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Liste de toutes les annotations\n",
    "train_captions = []\n",
    "# Liste de tous les noms de fichiers des images dupliquées (en nombre d'annotations par image)\n",
    "img_name_vector = []\n",
    "\n",
    "for image_path in train_image_paths:\n",
    "    caption_list = image_path_to_caption[image_path]\n",
    "    # Rajout de caption_list dans train_captions\n",
    "    train_captions.extend(caption_list)\n",
    "    # Rajout de image_path dupliquée len(caption_list) fois\n",
    "    img_name_vector.extend([image_path] * len(caption_list))\n",
    "    \n",
    "print(\"Préparation terminée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f032d8",
   "metadata": {},
   "source": [
    "## 3.7. Visuation d'une image annotée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2585afaa85f28d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T06:56:02.884197800Z",
     "start_time": "2023-10-26T06:56:02.750015200Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(train_captions), len(img_name_vector))\n",
    "print(train_captions[0])\n",
    "Image.open(img_name_vector[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b4185d8a144ae4",
   "metadata": {},
   "source": [
    "# 4. Prétraitement des images et des annotations\n",
    "\n",
    "## 4.1. Utilisation du modèle InceptionV3\n",
    "\n",
    "On va s'appuyer la puissance des modèles pré-entraînés pour faciliter notre tâche de légendage d'images. Nous utilisons le modèle InceptionV3 de Google, largement reconnu pour ses performances en matière de classification d'images. Initialement formé sur le dataset ImageNet, ce modèle a appris à identifier et à extraire des caractéristiques essentielles de millions d'images. En utilisant ce modèle pré-entraîné, nous pouvons bénéficier de cette capacité d'extraction sans avoir à former un modèle à partir de zéro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6e791ab93361c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T06:56:07.196167500Z",
     "start_time": "2023-10-26T06:56:02.872520200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Telechargement du modèle InceptionV3 pré-entrainé avec la cassification sur ImageNet\n",
    "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "# Creation d'une variable qui sera l'entrée du nouveau modèle de pre-traitement d'images\n",
    "new_input = image_model.input\n",
    "# récupérer la dernière couche caché qui contient l'image en representation compacte\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "# Modèle qui calcule une representation dense des images avec InceptionV3\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
    "\n",
    "print(\"Modèle téléchargé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e116a60",
   "metadata": {},
   "source": [
    "## 4.2. Traitement des images\n",
    "\n",
    "La fonction `load_image` est conçue pour traiter et préparer une image pour la phase d'entraînement ou de prédiction. Elle commence par charger l'image depuis le chemin d'accès spécifié. Une fois l'image chargée, plusieurs étapes de traitement sont appliquées :\n",
    "\n",
    "* L'image est d'abord décodée en format RGB, ce qui signifie qu'elle sera représentée en trois canaux : Rouge, Vert et Bleu.\n",
    "* L'image est ensuite redimensionnée pour avoir une taille uniforme de 299×299 pixels (dimension attendue par le modèle).\n",
    "* L'image est normalisée. Au lieu d'avoir des valeurs de pixels allant de 0 à 255, la normalisation ajuste ces valeurs pour qu'elles se situent entre -1 et 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b34652ff4aea9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T06:56:07.208067300Z",
     "start_time": "2023-10-26T06:56:07.201149600Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\"\n",
    "    La fonction load_image a pour entrée le chemin d'une image et pour sortie un couple\n",
    "    contenant l'image traitée ainsi que son chemin d'accès.\n",
    "    La fonction load_image effectue les traitement suivant:\n",
    "        1. Chargement du fichier correspondant au chemin d'accès image_path\n",
    "        2. Décodage de l'image en RGB.\n",
    "        3. Redimensionnement de l'image en taille (299, 299).\n",
    "        4. Normalisation des pîxels de l'image entre -1 et 1\n",
    "    \"\"\"\n",
    "    # Lecture du fichier image\n",
    "    img = tf.io.read_file(image_path)\n",
    "    # Decodage de l'image en RGB\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Redimensionnement de l'image en taille (299, 299)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    # Normalisation des pîxels de l'image entre -1 et 1\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path\n",
    "\n",
    "print('load_image défini.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd128e2b",
   "metadata": {},
   "source": [
    "## 4.3. Extraction des caractéristiques des images\n",
    "\n",
    "On commence par créer un dataset d'images. Pour chaque batch d'images, on utilise le modèle InceptionV3 pour transformer chaque image en une version compacte appelée \"représentation dense\". Ces représentations sont ensuite sauvegardées pour une utilisation ultérieure, et le nom original de l'image est utilisé pour nommer le fichier de la représentation dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253f8c1d8acdf094",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T06:57:57.231244600Z",
     "start_time": "2023-10-26T06:56:07.208659300Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Pré-traitement des images\n",
    "# Prendre les noms des images\n",
    "encode_train = sorted(set(img_name_vector))\n",
    "\n",
    "# Creation d'une instance de \"tf.data.Dataset\" partant des noms des images \n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "# Division du données en batchs après application du pré-traitement fait par load_image\n",
    "image_dataset = image_dataset.map(\n",
    "    load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
    "\n",
    "# Parcourir le dataset batch par batch pour effectuez le pré-traitement d'InceptionV3\n",
    "for img, path in tqdm(image_dataset):\n",
    "    # Pré-traitement du batch (de taille (16,8,8,2048)) courant par InceptionV3 \n",
    "    batch_features = image_features_extract_model(img)\n",
    "    # Resize du batch de taille (16,8,8,2048) en taille (16,64,2048)\n",
    "    batch_features = tf.reshape(batch_features,\n",
    "                                (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    # Parcourir le batch courant et stocker le chemin ainsi que le batch avec np.save()\n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        # (chemin de l'image associe a sa nouvelle representation , representation de l'image)\n",
    "        np.save(\n",
    "            path_of_feature, bf.numpy()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ef24d8e5d6e205",
   "metadata": {},
   "source": [
    "## 4.4. Prétraitement des annotations\n",
    "\n",
    "**Tokenisation et séquençage :**\n",
    "\n",
    "Les annotations textuelles, qui décrivent le contenu des images, doivent être converties en un format que le modèle peut comprendre. Pour ce faire, on utilise un tokenizer. Le tokenizer transforme chaque mot des annotations en un numéro unique (token). Dans ce code, on limite le vocabulaire aux 5 000 mots les plus fréquents pour gérer la complexité.\n",
    "\n",
    "Après avoir construit le vocabulaire à partir des annotations, un token spécial <pad> est ajouté. Ce token sera utilisé pour égaliser la longueur des séquences, car les modèles de traitement du langage naturel nécessitent des entrées de taille fixe.\n",
    "\n",
    "**Padding des séquences :**\n",
    "    \n",
    "Après la tokenisation, chaque annotation est convertie en une séquence de tokens. Cependant, comme les annotations ont des longueurs variables, on utilise le padding pour s'assurer que toutes les séquences ont la même longueur, celle de l'annotation la plus longue.\n",
    "\n",
    "**Sauvegarde du tokenizer :**\n",
    "    \n",
    "Le tokenizer, qui détient le mappage entre les mots et leurs tokens, est essentiel pour la suite du processus. Pour éviter de le recalculer à chaque fois, nous le sauvegardons sous forme de fichier JSON. Cela permettra de le recharger plus tard, par exemple lors de l'évaluation du modèle ou de son déploiement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d222e2ce67122a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T06:57:58.254830Z",
     "start_time": "2023-10-26T06:57:57.233622300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Trouver la taille maximale \n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "# Chosir les 5000 mots les plus frequents du vocabulaire\n",
    "top_k = 5000\n",
    "#La classe Tokenizer permet de faire du pre-traitement de texte pour reseau de neurones \n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "# Construit un vocabulaire en se basant sur la liste train_captions\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "\n",
    "# Créer le token qui sert à remplir les annotations pour egaliser leurs longueur\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "# Creation des vecteurs(liste de token entiers) à partir des annotations (liste de mots)\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "\n",
    "# Remplir chaque vecteur à jusqu'à la longueur maximale des annotations\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "\n",
    "# Calcule la longueur maximale qui est utilisée pour stocker les poids d'attention \n",
    "# Elle servira plus tard pour l'affichage lors de l'évaluation\n",
    "max_length = calc_max_length(train_seqs)\n",
    "\n",
    "# Convert the tokenizer to a JSON string\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "\n",
    "# Save the JSON string to a file\n",
    "with open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "    \n",
    "print('Annotations prétraités.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3643fcb93d42662d",
   "metadata": {},
   "source": [
    "# 5. Préparation et configuration des données pour l'entraînement\n",
    "\n",
    "## 5.1 Formation du jeu d'entrainement et de test\n",
    "\n",
    "On a opté pour une répartition de 80% des données pour l'entraînement et 20% pour le test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a9a719b9185c41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T06:57:58.300579300Z",
     "start_time": "2023-10-26T06:57:58.269248400Z"
    }
   },
   "outputs": [],
   "source": [
    "img_to_cap_vector = collections.defaultdict(list)\n",
    "# Creation d'un dictionnaire associant les chemins des images avec (fichier .npy) aux annotationss\n",
    "# Les images sont dupliquées car il y a plusieurs annotations par image\n",
    "print(len(img_name_vector), len(cap_vector))\n",
    "for img, cap in zip(img_name_vector, cap_vector):\n",
    "    img_to_cap_vector[img].append(cap)\n",
    "\n",
    "\"\"\"\n",
    "Création des datasets de formation et de validation en utilisant \n",
    "un fractionnement 80-20 de manière aléatoire\n",
    "\"\"\"\n",
    "# Prendre les clés (noms des fichiers d'images traites), *celles-ci ne seront pas dupliquées*\n",
    "img_keys = list(img_to_cap_vector.keys())\n",
    "random.shuffle(img_keys) # Mélanger les clés\n",
    "# Diviser des indices en entrainement et test\n",
    "slice_index = int(len(img_keys)*0.8)\n",
    "# divide into train and validation\n",
    "img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n",
    "\n",
    "\"\"\"\n",
    "Les jeux d'entrainement et de tests sont sous forme\n",
    "de listes contenants les mappings :(image prétraitée ---> jeton d'annotation(mot) )\n",
    "\"\"\"\n",
    "\n",
    "# Boucle pour construire le jeu d'entrainement\n",
    "img_name_train = []\n",
    "cap_train = []\n",
    "for imgt in img_name_train_keys:\n",
    "    capt_len = len(img_to_cap_vector[imgt])\n",
    "    # Duplication des images en le nombre d'annotations par image\n",
    "    img_name_train.extend([imgt] * capt_len)\n",
    "    cap_train.extend(img_to_cap_vector[imgt])\n",
    "\n",
    "# Boucle pour construire le jeu de test\n",
    "img_name_val = []\n",
    "cap_val = []\n",
    "for imgv in img_name_val_keys:\n",
    "    capv_len = len(img_to_cap_vector[imgv])\n",
    "    # Duplication des images en le nombre d'annotations par image\n",
    "    img_name_val.extend([imgv] * capv_len)\n",
    "    cap_val.extend(img_to_cap_vector[imgv])\n",
    "\n",
    "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63a3756",
   "metadata": {},
   "source": [
    "## 5.2 Configuration du dataset pour l'entraînement\n",
    "\n",
    "**Paramétrage Initial :**\n",
    "\n",
    "On définit plusieurs paramètres essentiels pour l'entraînement, tels que la taille du batch (`BATCH_SIZE`), la taille du buffer pour mélanger les données (`BUFFER_SIZE`), la dimension de l'embedding (`embedding_dim`), la taille de la couche cachée du RNN (`units`) et la taille du vocabulaire (`vocab_size`). De plus, on calcule le nombre d'étapes par époque en fonction de la taille du batch et du nombre d'images d'entraînement.\n",
    "\n",
    "**Préparation du Dataset :**\n",
    "\n",
    "L'étape suivante consiste à créer un dataset TensorFlow à partir des noms des fichiers d'images traitées et des annotations. Une fonction, `map_func`, est définie pour charger les fichiers numpy correspondant à chaque image et renvoyer le tensor de l'image avec son annotation.\n",
    "\n",
    "Le dataset est ensuite configuré pour charger ces tensors en utilisant la fonction map_func. La méthode shuffle est utilisée pour mélanger les données, ce qui est essentiel pour garantir que le modèle ne mémorise pas l'ordre des données et généralise bien. Enfin, `prefetch` est utilisé pour améliorer les performances en préchargeant les données pendant l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff2a5c9f3f44e50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T06:57:58.872801300Z",
     "start_time": "2023-10-26T06:57:58.301621700Z"
    }
   },
   "outputs": [],
   "source": [
    "# N'hésitez pas à modifier ces paramètres en fonction de votre machine\n",
    "BATCH_SIZE = 64 # taille du batch\n",
    "BUFFER_SIZE = 1000 # taille du buffer pour melanger les donnes\n",
    "embedding_dim = 256\n",
    "units = 512 # Taille de la couche caché dans le RNN\n",
    "vocab_size = top_k + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "\n",
    "# La forme du vecteur extrait à partir d'InceptionV3 est (64, 2048)\n",
    "# Les deux variables suivantes representent la forme de ce vecteur\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64\n",
    "\n",
    "# Fonction qui charge les fichiers numpy des images prétraitées\n",
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    return img_tensor, cap\n",
    "\n",
    "# Creation d'un dataset de \"Tensor\"s (sert à representer de grands dataset)\n",
    "# Le dataset est cree a partir de \"img_name_train\" et \"cap_train\"\n",
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# L'utilisation de map permet de charger les fichiers numpy (possiblement en parallèle)\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "    map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Melanger les donnees et les diviser en batchs\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "print('Dataset configuré.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a3b6b7bf5aee0",
   "metadata": {},
   "source": [
    "# 6. Le modèle\n",
    "\n",
    "## 6.1. Encodeur CNN\n",
    "\n",
    "L'encodeur CNN est conçu pour traiter les caractéristiques extraites des images par le modèle InceptionV3. Ces caractéristiques, qui sont une représentation dense de l'image, sont passées à travers cet encodeur pour être utilisées dans le modèle de légendage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b10b31ce60a1025",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T06:57:58.884503100Z",
     "start_time": "2023-10-26T06:57:58.876953Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Comme les images sont déjà prétraités par InceptionV3 est représenté sous forme compacte\n",
    "    # L'encodeur CNN ne fera que transmettre ces caractéristiques à une couche dense\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # forme après fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n",
    "\n",
    "print('Encodeur CNN configuré.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01289a6",
   "metadata": {},
   "source": [
    "## 6.2. Mécanisme d'attention de Bahdanau\n",
    "\n",
    "Ce modèle implémente le mécanisme d'attention de Bahdanau, qui est largement utilisé dans les tâches de traitement du langage naturel pour permettre au modèle de se \"concentrer\" sur différentes parties des données d'entrée lors de la génération de la sortie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2522d285d19c8ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T06:57:58.928557400Z",
     "start_time": "2023-10-26T06:57:58.882950Z"
    }
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) forme == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # forme de la couche cachée == (batch_size, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
    "                                             self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # Cela vous donne un score non normalisé pour chaque caractéristique de l'image.\n",
    "        score = self.V(attention_hidden_layer)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "print(\"Mécanisme d'attention configuré.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6956f9e",
   "metadata": {},
   "source": [
    "## 6.3. Décodeur RNN\n",
    "\n",
    "Ce décodeur utilise un RNN (spécifiquement une GRU) pour générer des légendes à partir des représentations denses des images. Il intègre également un mécanisme d'attention pour aider le RNN à se concentrer sur différentes parties de l'image à différentes étapes de la génération de la légende."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ef424e50efad84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T06:57:58.929061900Z",
     "start_time": "2023-10-26T06:57:58.904658600Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        #Couche dense qui aura pour entrée la sortie du GRU\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        # Dernière couche dense\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # L'attention est defini par un modèle a part\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "        # Passage du mot courant à la couche embedding\n",
    "        x = self.embedding(x)\n",
    "        # Concaténation\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # Passage du vecteur concaténé à la gru\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # Couche dense\n",
    "        y = self.fc1(output)\n",
    "\n",
    "        y = tf.reshape(y, (-1, x.shape[2]))\n",
    "\n",
    "        # Couche dense\n",
    "        y = self.fc2(y)\n",
    "\n",
    "        return y, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))\n",
    "    \n",
    "print('Décodeur RNN configuré.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bd1950bf0921de",
   "metadata": {},
   "source": [
    "## 6.4. Combinaison de la la partie encodeur et décodeur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4994cc529948a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T06:57:58.967993700Z",
     "start_time": "2023-10-26T06:57:58.919359500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Création de l'encodeur\n",
    "encoder = CNN_Encoder(embedding_dim)\n",
    "# Création du décodeur\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
    "\n",
    "print('Encodeur et décodeur combiné.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d8476a",
   "metadata": {},
   "source": [
    "## 6.5. Configuration de l'optimisation et de la fonction de loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21b2d4647a794b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T06:57:58.981475700Z",
     "start_time": "2023-10-26T06:57:58.970188500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Optimiseur ADAM\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "# La fonction de perte\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "print(\"Configuration de l'optimisation terminé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40abb86efead653e",
   "metadata": {},
   "source": [
    "## 6.6. Mise en place du checkpoint\n",
    "\n",
    "Pour garder la trace de notre apprentissage et la sauvegarder, on va utiliser la classe [`tf.train.Checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b2eef3e1809fdc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-26T06:57:58.977683Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.abspath(\"checkpoints\")\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "print(\"Checkpoint défini.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e8ef81f6828763",
   "metadata": {},
   "source": [
    "## 6.7. Initialisation du checkpoint\n",
    "\n",
    "Initialisation de l'époque de début d’entrainement dans `start_epoch`. La classe `tf.train.Checkpoint` permet de poursuivre l’entrainement là ou vous l’avez laissé s’il avait été interrompu auparavant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbc3649e72526ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T06:57:59.019224100Z",
     "start_time": "2023-10-26T06:57:58.992695300Z"
    }
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    # Restaurer le dernier checkpoint dans checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    \n",
    "print(\"Checkpoint initialisé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec8dc0626347584",
   "metadata": {},
   "source": [
    "# 7. Entrainement et test :\n",
    "\n",
    "## 7.1 Définition de l'entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fa30cf8095069f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T06:57:59.028796800Z",
     "start_time": "2023-10-26T06:57:59.013686Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_plot = []\n",
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # Initialisation de l'état caché pour chaque batch\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    # Initialiser l'entrée du décodeur\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape: # Offre la possibilité de calculer le gradient du loss\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # Prédiction des i'èmes mot du batch avec le décodeur\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # Le mot correct à l'étap i est donné en entrée à l'étape (i+1)\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c509565a7b094c",
   "metadata": {},
   "source": [
    "## 7.2 Entraînement du modèle\n",
    "\n",
    "Le code global contenant la boucle d'entrainement est présenté ci-dessous. Cette boucle parcours le jeu de données d'entrainement batch par batch et entraine le réseaux avec ceux-ci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543b1fdc111a7395",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-26T06:57:59.030960800Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "    # sauvegarde de la perte\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "# Affichage de la courbe d'entrainement\n",
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab73ac53c58c555b",
   "metadata": {},
   "source": [
    "## 7.3 Définition de la fonction de test du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c479ead4f70edb69",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot\n",
    "\n",
    "# Fonction permettant la représentation de l'attention au niveau de l'image\n",
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41908be047f422fe",
   "metadata": {},
   "source": [
    "## 7.4 Test du modèle\n",
    "\n",
    "L'affichage de quelques exemples sur le résultat retourné par l'évaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c24de6350377840",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Affichage de quelques annotations dans le jeu de test\n",
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "print(rid)\n",
    "print(len(cap_val))\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(image, result, attention_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a6b76bb9f13e76",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "Image.open(img_name_val[rid])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
